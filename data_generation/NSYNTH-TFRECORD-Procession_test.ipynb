{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[tfrecord tutorial from TensorFlow](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import tensorflow_datasets as tfds\n",
    "import os\n",
    "#from tensorflow_datasets.audio import nsynth\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToJson"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PATH_TRAIN = r\"E:\\nsynth233\\2.3.3\\train\"\n",
    "PATH_VALID = r\"E:\\nsynth233\\2.3.3\\valid\"\n",
    "PATH_TEST = r\"E:\\nsynth233\\2.3.3\\test\"\n",
    "#TFRECORD_FILES_TRAIN = os.listdir(PATH_TRAIN)\n",
    "#TFRECORD_FILES_VALIDATION = os.listdir(PATH_VALID)\n",
    "TFRECORD_FILES_TEST = os.listdir(PATH_TEST)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nsynth._AUDIO_RATE\n",
    "nsynth._DESCRIPTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nsynth._SPLITS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect single TFR example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## gansynth subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(r\"E:\\nsynth233\\2.3.3\\train\\nsynth-train.tfrecord-00000-of-00128\")\n",
    "for d in dataset.take(1):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    ex2 = ex.ParseFromString(d.numpy())\n",
    "    gan_ex = json.loads(MessageToJson(ex))\n",
    "    #print(m['features']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex3 = ex.context.feature\n",
    "ex3[\"audio\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(gan_ex['context']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gan_ex['context']['feature']['f0/hz']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gan_ex['context']['feature']['f0/confidence']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gan_ex['context']['feature']['loudness/db']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gan_ex['context']['feature']['f0/midi']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gan_ex['context'][\"feature\"][\"audio\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.3 full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(r\"datasets_nsynth_full_2.3.3_nsynth-test (1).tfrecord-00000-of-00008\")\n",
    "for d in dataset.take(1):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    ex2 = ex.ParseFromString(d.numpy())\n",
    "    m = json.loads(MessageToJson(ex))\n",
    "    #print(m['features']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex3 = ex.context.feature\n",
    "ex3[\"audio\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(m['context']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m['context']['feature']['loudness/db']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m['context'][\"feature\"][\"audio\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and Write TFR Files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "take some random example to write"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "files_to_write = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for raw_record in dataset.take(2):\n",
    "    files_to_write.append(raw_record)\n",
    "    example_write = tf.train.SequenceExample()\n",
    "    write_to_disk = raw_record\n",
    "\n",
    "#print(write_to_disk)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(other way to represent it)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) add the tensor file to tf.data.Dataset (more could be added)\n",
    "2) write the file to some filename (it should now be visible in your folder structure)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filename_example = \"nsynth-valid-nico.tfrecord-111-of-32\"\n",
    "for idx,file in  enumerate(files_to_write):\n",
    "    dataset_write = tf.data.Dataset.from_tensors(file)\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename_example)\n",
    "    writer.write(dataset_write)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_write = tf.data.Dataset.from_tensor_slices(files_to_write)\n",
    "writer = tf.data.experimental.TFRecordWriter(filename_example )\n",
    "writer.write(dataset_write)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "raw_dataset_read = tf.data.TFRecordDataset(filename_example)\n",
    "print(raw_dataset_read)\n",
    "for raw_record_nico in raw_dataset_read.take(2):\n",
    "    read_from_disk = raw_record_nico\n",
    "    #print(read_from_disk)\n",
    "    example_read = tf.train.SequenceExample()\n",
    "    print(example_read.ParseFromString(read_from_disk.numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset_read.take(2):\n",
    "\n",
    "    # extract instrument family\n",
    "    example = tf.train.SequenceExample()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    example_JSON = json.loads(MessageToJson(example))\n",
    "    instrument_family = example_JSON[\"context\"][\"feature\"]['instrument/family']['int64List']['value']\n",
    "\n",
    "    #['feature']['instrument/family']['int64List']['value']\n",
    "    print(instrument_family)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "filename = f\"nsynth-valid-nico.tfrecord-00-of-32\"\n",
    "dataset_write = tf.data.Dataset.from_tensor_slices(files_to_write)\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(dataset_write)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "read the same file back in and check if it is really the same"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_read.ParseFromString(read_from_disk.numpy()) == example_write.ParseFromString(write_to_disk.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "so the two files represent exactly the same object !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sorting by instrument family"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation Files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_samples_info = []\n",
    "original_order_instr_fam = []\n",
    "for group_index,data_group in enumerate(TFRECORD_FILES_VALIDATION):\n",
    "  data_group = tf.data.TFRecordDataset(PATH_VALID + \"/\" + data_group)\n",
    "  print(group_index)\n",
    "  for raw_record in data_group:\n",
    "    if group_index<=20:\n",
    "      parsed_record = tf.train.SequenceExample()\n",
    "      parsed_record.ParseFromString(raw_record.numpy())\n",
    "      # convert raw uninformative string to JSON\n",
    "      for_inst_fam = json.loads(MessageToJson(ex))\n",
    "      instrument_family = for_inst_fam['context']['feature']['instrument/family']['int64List']['value']\n",
    "      #instrument_type = parsed_record.features.feature['instrument_family_str'].bytes_list.value[0]\n",
    "      original_order_instr_fam.append(instrument_family)\n",
    "      processed_samples_info.append([group_index,raw_record,instrument_family,parsed_record])\n",
    "      #print(instrument_type)\n",
    "    else:\n",
    "        print(\"finished\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex = parsed_record\n",
    "m = json.loads(MessageToJson(ex))\n",
    "\n",
    "print(m['context']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(m['context']['feature']['instrument/family']['int64List']['value'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "unique_fam_values = set([fam[0] for fam in original_order_instr_fam])\n",
    "print(unique_fam_values)\n",
    "# looks like all validation samples belong to instr family 5, what about instr label ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instrument_families = [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\",\"mallet\",\"organ\",\"reed\",\"string\",\"synth_lead\",\"vocal\"]\n",
    "sorted_by_instr_val = {\n",
    "\"bass\": [],\n",
    "\"brass\": [],\n",
    "\"flute\" : [],\n",
    "\"guitar\" : [],\n",
    "\"keyboard\" : [],\n",
    "\"mallet\" : [],\n",
    "\"organ\" : [],\n",
    "\"reed\" : [],\n",
    "\"string\" : [],\n",
    "\"synth_lead\" : [],\n",
    "\"vocal\" : []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def sort_by_instrument(data, instr_dict):\n",
    "    for sample in data:\n",
    "      instr_fam = sample[2]\n",
    "      print(instr_fam)\n",
    "      instr_dict[instrument_families[int(instr_fam[0])]].append(sample)\n",
    "    return instr_dict\n",
    "\n",
    "\n",
    "sorted_by_instr_val  = sort_by_instrument(processed_samples_info, sorted_by_instr_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_by_instr_val[\"guitar\"][3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "guitar = sorted_by_instr_val[\"guitar\"][3]\n",
    "#guitar\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"bass\": [],\n",
    "    \"brass\": [],\n",
    "    \"flute\" : [],\n",
    "    \"guitar\" : [[0,\"tensor\",\"type\",\"parsed\"]],\n",
    "    \"keyboard\" : [],\n",
    "    \"mallet\" : [],\n",
    "    \"organ\" : [],\n",
    "    \"reed\" : [],\n",
    "    \"string\" : [],\n",
    "    \"synth_lead\" : [],\n",
    "    \"vocal\" : []\n",
    "}\n",
    "\n",
    "test_guitar = test_dict[\"guitar\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_guitar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# same as last one (todo ?)\n",
    "def sort_raw_records_by_instrument(data):\n",
    "    instrument_families = [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\",\"mallet\",\"organ\",\"reed\",\"string\",\"synth_lead\",\"vocal\"]\n",
    "    instr_dict_train = {\n",
    "        \"bass\": [],\n",
    "        \"brass\": [],\n",
    "        \"flute\" : [],\n",
    "        \"guitar\" : [],\n",
    "        \"keyboard\" : [],\n",
    "        \"mallet\" : [],\n",
    "        \"organ\" : [],\n",
    "        \"reed\" : [],\n",
    "        \"string\" : [],\n",
    "        \"synth_lead\" : [],\n",
    "        \"vocal\" : []\n",
    "    }\n",
    "    for sample in data:\n",
    "      instr_fam = sample[1]\n",
    "      #print(instr_fam)\n",
    "      instr_dict_train[instrument_families[int(instr_fam[0])]].append(sample[0])\n",
    "    return instr_dict_train\n",
    "\n",
    "def writeTFRTrainFiles(instr_dict_train,sort_round):\n",
    "    for key,instr_values in instr_dict_train.items():\n",
    "        if instr_values:\n",
    "            filename_train = f\"{str(sort_round)}-train-{str(key)}-nsynth-subset-train.tfrecord-{str(sort_round)}-label-{str(key)}\"\n",
    "            dataset_write = tf.data.Dataset.from_tensor_slices(instr_values)\n",
    "            writer = tf.data.experimental.TFRecordWriter(filename_train)\n",
    "            writer.write(dataset_write)\n",
    "        else:\n",
    "            print(key,\" has no instrument values for sort round \",sort_round)\n",
    "    return None\n",
    "\n",
    "#sorted_by_instr_train  = sort_raw_records_by_instrument(processed_samples_info_train, sorted_by_instr_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "upper = 20\n",
    "lower = -1\n",
    "\n",
    "#sort_round = 0,1\n",
    "sort_round = 2\n",
    "processed_samples_info_train = []\n",
    "\n",
    "STARTPOSITION = 38\n",
    "\n",
    "for source_group_index,file_in_name in enumerate(TFRECORD_FILES_TRAIN[STARTPOSITION:]):\n",
    "  data_group_train = tf.data.TFRecordDataset(PATH_TRAIN + \"/\" + file_in_name)\n",
    "  #original_order_instr_fam_train = []\n",
    "  if lower < source_group_index < upper: # adjusts per round (>= act / <= act+20)\n",
    "      print(source_group_index)\n",
    "      for raw_record in data_group_train:\n",
    "          parsed_record = tf.train.SequenceExample()\n",
    "          parsed_record.ParseFromString(raw_record.numpy())\n",
    "          # convert raw uninformative string to JSON and extract relevant features\n",
    "          processed = json.loads(MessageToJson(parsed_record))\n",
    "          instrument_family = processed['context']['feature']['instrument/family']['int64List']['value']\n",
    "          #instrument_label = processed['context']['feature']['instrument/label']\n",
    "          #f0_hz = processed['context']['feature']['f0/hz']\n",
    "          #f0_confidence = processed['context']['feature']['f0/confidence']\n",
    "          #loudness_decibel = processed['context']['feature']['loudness/db']\n",
    "          #audio = processed['context'][\"feature\"][\"audio\"]\n",
    "          #instrument_type = parsed_record.features.feature['instrument_family_str'].bytes_list.value[0]\n",
    "          #original_order_instr_fam_train.append((instrument_family,instrument_label))\n",
    "          processed_samples_info_train.append([raw_record,instrument_family])\n",
    "  else:\n",
    "      # write files to disk and clean memory\n",
    "      upper +=20\n",
    "      lower +=20\n",
    "      instr_dict_train = sort_raw_records_by_instrument(processed_samples_info_train)\n",
    "      writeTFRTrainFiles(instr_dict_train, sort_round)\n",
    "      del instr_dict_train\n",
    "      del processed_samples_info_train\n",
    "      print(f\"round {sort_round} finished\")\n",
    "      sort_round += 1\n",
    "      \n",
    "      # continue\n",
    "      for raw_record in data_group_train:\n",
    "          parsed_record = tf.train.SequenceExample()\n",
    "          parsed_record.ParseFromString(raw_record.numpy())\n",
    "          processed = json.loads(MessageToJson(parsed_record))\n",
    "          instrument_family = processed['context']['feature']['instrument/family']['int64List']['value']\n",
    "          processed_samples_info_train = []\n",
    "          processed_samples_info_train.append((raw_record,instrument_family))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex = parsed_record\n",
    "m = json.loads(MessageToJson(ex))\n",
    "\n",
    "print(m['context']['feature'].keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(m['context']['feature']['instrument/family']['int64List']['value'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "unique_fam_values = set([fam[0] for fam in original_order_instr_fam])\n",
    "print(unique_fam_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# same as last one (todo ?)\n",
    "def sort_raw_records_by_instrument(data):\n",
    "    instrument_families = [\"bass\", \"brass\", \"flute\", \"guitar\", \"keyboard\",\"mallet\",\"organ\",\"reed\",\"string\",\"synth_lead\",\"vocal\"]\n",
    "    instr_dict_test = {\n",
    "        \"bass\": [],\n",
    "        \"brass\": [],\n",
    "        \"flute\" : [],\n",
    "        \"guitar\" : [],\n",
    "        \"keyboard\" : [],\n",
    "        \"mallet\" : [],\n",
    "        \"organ\" : [],\n",
    "        \"reed\" : [],\n",
    "        \"string\" : [],\n",
    "        \"synth_lead\" : [],\n",
    "        \"vocal\" : []\n",
    "    }\n",
    "    for sample in data:\n",
    "      instr_fam = sample[1]\n",
    "      #print(instr_fam)\n",
    "      instr_dict_test[instrument_families[int(instr_fam[0])]].append(sample[0])\n",
    "    return instr_dict_test\n",
    "\n",
    "def writeTFRTestFiles(instr_dict_test,sort_round):\n",
    "    for key,instr_values in instr_dict_test.items():\n",
    "        if instr_values:\n",
    "            filename_test = f\"{str(sort_round)}-test-{str(key)}-nsynth-subset-test.tfrecord-{str(sort_round)}-label-{str(key)}\"\n",
    "            dataset_write = tf.data.Dataset.from_tensor_slices(instr_values)\n",
    "            writer = tf.data.experimental.TFRecordWriter(filename_test)\n",
    "            writer.write(dataset_write)\n",
    "        else:\n",
    "            print(key,\" has no instrument values for sort round \",sort_round)\n",
    "    return None\n",
    "\n",
    "#sorted_by_instr_train  = sort_raw_records_by_instrument(processed_samples_info_train, sorted_by_instr_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "upper = 20\n",
    "lower = -1\n",
    "\n",
    "\n",
    "sort_round = 0\n",
    "processed_samples_info_test = []\n",
    "\n",
    "STARTPOSITION = 0\n",
    "\n",
    "for source_group_index,file_in_name in enumerate(TFRECORD_FILES_TEST[STARTPOSITION:]):\n",
    "  data_group_test = tf.data.TFRecordDataset(PATH_TEST + \"/\" + file_in_name)\n",
    "  #original_order_instr_fam_train = []\n",
    "  if lower < source_group_index < upper: # adjusts per round (>= act / <= act+20)\n",
    "      print(source_group_index)\n",
    "      for raw_record in data_group_test:\n",
    "          parsed_record = tf.train.SequenceExample()\n",
    "          parsed_record.ParseFromString(raw_record.numpy())\n",
    "          # convert raw uninformative string to JSON and extract relevant features\n",
    "          processed = json.loads(MessageToJson(parsed_record))\n",
    "          instrument_family = processed['context']['feature']['instrument/family']['int64List']['value']\n",
    "          #instrument_label = processed['context']['feature']['instrument/label']\n",
    "          #f0_hz = processed['context']['feature']['f0/hz']\n",
    "          #f0_confidence = processed['context']['feature']['f0/confidence']\n",
    "          #loudness_decibel = processed['context']['feature']['loudness/db']\n",
    "          #audio = processed['context'][\"feature\"][\"audio\"]\n",
    "          #instrument_type = parsed_record.features.feature['instrument_family_str'].bytes_list.value[0]\n",
    "          #original_order_instr_fam_train.append((instrument_family,instrument_label))\n",
    "          processed_samples_info_test.append([raw_record,instrument_family])\n",
    "  else:\n",
    "      # write files to disk and clean memory\n",
    "      upper +=20\n",
    "      lower +=20\n",
    "      instr_dict_test = sort_raw_records_by_instrument(processed_samples_info_test)\n",
    "      writeTFRTestFiles(instr_dict_test, sort_round)\n",
    "      del instr_dict_test\n",
    "      del processed_samples_info_test\n",
    "      print(f\"round {sort_round} finished\")\n",
    "      sort_round += 1\n",
    "\n",
    "      # continue\n",
    "      for raw_record in data_group_test:\n",
    "          parsed_record = tf.train.SequenceExample()\n",
    "          parsed_record.ParseFromString(raw_record.numpy())\n",
    "          processed = json.loads(MessageToJson(parsed_record))\n",
    "          instrument_family = processed['context']['feature']['instrument/family']['int64List']['value']\n",
    "          processed_samples_info_test = []\n",
    "          processed_samples_info_test.append((raw_record,instrument_family))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "mallet_path_train = r\"E:\\nsynthgan_generation_sorted\\mallet\\train\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "my_batch_size = 126\n",
    "filenames_full = glob.glob(r\"E:\\nsynthgan_generation_sorted\\mallet\\train\\*.tfrecord-*\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ddsp in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: tflite-support in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.1.0a1)\n",
      "Requirement already satisfied: mir-eval in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.7)\n",
      "Requirement already satisfied: crepe<=0.0.12 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.0.12)\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (4.7.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (1.0.0)\n",
      "Requirement already satisfied: cloudml-hypertune in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.1.0.dev6)\n",
      "Requirement already satisfied: tensorflowjs<3.19 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (3.18.0)\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.18.0)\n",
      "Requirement already satisfied: hmmlearn<=0.2.7 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.2.7)\n",
      "Requirement already satisfied: six in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (1.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.18.2)\n",
      "Requirement already satisfied: protobuf<=3.20.* in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from ddsp) (3.19.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (1.22.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from ddsp) (2.11.0)\n",
      "Requirement already satisfied: gin-config>=0.3.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.5.0)\n",
      "Requirement already satisfied: tensorflow-probability in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.18.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.9.2)\n",
      "Requirement already satisfied: note-seq<0.0.4 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.0.3)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (1.29.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from ddsp) (0.25.1)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from crepe<=0.0.12->ddsp) (1.0.2)\n",
      "Requirement already satisfied: resampy<0.3.0,>=0.2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from crepe<=0.0.12->ddsp) (0.2.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from crepe<=0.0.12->ddsp) (2.19.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from crepe<=0.0.12->ddsp) (3.5.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from crepe<=0.0.12->ddsp) (3.6.0)\n",
      "Requirement already satisfied: bokeh>=0.12.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (3.0.0)\n",
      "Requirement already satisfied: intervaltree>=2.1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (3.1.0)\n",
      "Requirement already satisfied: pretty-midi>=0.2.6 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (0.2.9)\n",
      "Requirement already satisfied: attrs in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (21.4.0)\n",
      "Requirement already satisfied: IPython in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (8.2.0)\n",
      "Requirement already satisfied: pandas>=0.18.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from note-seq<0.0.4->ddsp) (1.3.5)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (0.55.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (20.9)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (3.0.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from librosa->ddsp) (0.10.3.post1)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflowjs<3.19->ddsp) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow->ddsp) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (1.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (14.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (65.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (4.2.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (0.28.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->ddsp) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-cloud-storage->ddsp) (1.5.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-cloud-storage->ddsp) (0.5.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-cloud-storage->ddsp) (1.35.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-addons->ddsp) (2.13.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (2.27.1)\n",
      "Requirement already satisfied: dill in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (0.3.1.1)\n",
      "Requirement already satisfied: promise in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (0.10.2)\n",
      "Requirement already satisfied: etils[epath] in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-datasets->ddsp) (0.9.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-probability->ddsp) (0.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-probability->ddsp) (2.2.0)\n",
      "Requirement already satisfied: pybind11>=2.4 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tflite-support->ddsp) (2.9.2)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (3.1.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (5.4.1)\n",
      "Requirement already satisfied: contourpy>=1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (1.0.6)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (9.0.1)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (6.1)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from bokeh>=0.12.0->note-seq<0.0.4->ddsp) (2022.9.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->ddsp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->ddsp) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->ddsp) (4.2.4)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->ddsp) (1.31.5)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from intervaltree>=2.1.0->note-seq<0.0.4->ddsp) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib>=2.1.0->crepe<=0.0.12->ddsp) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib>=2.1.0->crepe<=0.0.12->ddsp) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib>=2.1.0->crepe<=0.0.12->ddsp) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib>=2.1.0->crepe<=0.0.12->ddsp) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib>=2.1.0->crepe<=0.0.12->ddsp) (4.33.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from numba>=0.45.1->librosa->ddsp) (0.38.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas>=0.18.1->note-seq<0.0.4->ddsp) (2022.2.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pooch>=1.0->librosa->ddsp) (1.4.4)\n",
      "Requirement already satisfied: mido>=1.1.16 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pretty-midi>=0.2.6->note-seq<0.0.4->ddsp) (1.2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets->ddsp) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets->ddsp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets->ddsp) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets->ddsp) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn>=0.16->crepe<=0.0.12->ddsp) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from soundfile>=0.10.2->librosa->ddsp) (1.15.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from etils[epath]->tensorflow-datasets->ddsp) (3.10.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from etils[epath]->tensorflow-datasets->ddsp) (5.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (2.12.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (2.0.10)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.4.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from IPython->note-seq<0.0.4->ddsp) (0.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets->ddsp) (1.52.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->ddsp) (0.37.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->ddsp) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from jedi>=0.16->IPython->note-seq<0.0.4->ddsp) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq<0.0.4->ddsp) (2.1.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->note-seq<0.0.4->ddsp) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage->ddsp) (0.4.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (2.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (0.6.1)\n",
      "Requirement already satisfied: executing in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from stack-data->IPython->note-seq<0.0.4->ddsp) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from stack-data->IPython->note-seq<0.0.4->ddsp) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from stack-data->IPython->note-seq<0.0.4->ddsp) (0.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->ddsp) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ddsp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DataProvider(object):\n",
    "    \"\"\"Base class for returning a dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, frame_rate):\n",
    "        \"\"\"DataProvider constructor.\n",
    "        Args:\n",
    "          sample_rate: Sample rate of audio in the dataset.\n",
    "          frame_rate: Frame rate of features in the dataset.\n",
    "        \"\"\"\n",
    "        self._sample_rate = sample_rate\n",
    "        self._frame_rate = frame_rate\n",
    "\n",
    "    @property\n",
    "    def sample_rate(self):\n",
    "        \"\"\"Return dataset sample rate, must be defined in the constructor.\"\"\"\n",
    "        return self._sample_rate\n",
    "\n",
    "    @property\n",
    "    def frame_rate(self):\n",
    "        \"\"\"Return dataset feature frame rate, must be defined in the constructor.\"\"\"\n",
    "        return self._frame_rate\n",
    "\n",
    "    def get_dataset(self, shuffle):\n",
    "        \"\"\"A method that returns a tf.data.Dataset.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_batch(self,\n",
    "                  batch_size,\n",
    "                  shuffle=True,\n",
    "                  repeats=-1,\n",
    "                  drop_remainder=True):\n",
    "        \"\"\"Read dataset.\n",
    "        Args:\n",
    "          batch_size: Size of batch.\n",
    "          shuffle: Whether to shuffle the examples.\n",
    "          repeats: Number of times to repeat dataset. -1 for endless repeats.\n",
    "          drop_remainder: Whether the last batch should be dropped.\n",
    "        Returns:\n",
    "          A batched tf.data.Dataset.\n",
    "        \"\"\"\n",
    "        dataset = self.get_dataset(shuffle)\n",
    "        dataset = dataset.repeat(repeats)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        dataset = dataset.prefetch(buffer_size=_AUTOTUNE)\n",
    "        return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class TfdsProvider(DataProvider):\n",
    "    \"\"\"Base class for reading datasets from TensorFlow Datasets (TFDS).\"\"\"\n",
    "\n",
    "    def __init__(self, name, split, data_dir, sample_rate, frame_rate):\n",
    "        \"\"\"TfdsProvider constructor.\n",
    "        Args:\n",
    "          name: TFDS dataset name (with optional config and version).\n",
    "          split: Dataset split to use of the TFDS dataset.\n",
    "          data_dir: The directory to read TFDS datasets from. Defaults to\n",
    "            \"~/tensorflow_datasets\".\n",
    "          sample_rate: Sample rate of audio in the dataset.\n",
    "          frame_rate: Frame rate of features in the dataset.\n",
    "        \"\"\"\n",
    "        self._name = name\n",
    "        self._split = split\n",
    "        self._data_dir = data_dir\n",
    "        super().__init__(sample_rate, frame_rate)\n",
    "\n",
    "    def get_dataset(self, shuffle=True):\n",
    "        \"\"\"Read dataset.\n",
    "        Args:\n",
    "          shuffle: Whether to shuffle the input files.\n",
    "        Returns:\n",
    "          dataset: A tf.data.Dataset that reads from TFDS.\n",
    "        \"\"\"\n",
    "        return tfds.load(\n",
    "            self._name,\n",
    "            data_dir=self._data_dir,\n",
    "            split=self._split,\n",
    "            shuffle_files=shuffle,\n",
    "            download=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class NSynthTfds(TfdsProvider):\n",
    "    \"\"\"Parses features in the TFDS NSynth dataset.\n",
    "    If running on Cloud, it is recommended you set `data_dir` to\n",
    "    'gs://tfds-data/datasets' to avoid unnecessary downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 name='nsynth/gansynth_subset.f0_and_loudness:2.3.0',\n",
    "                 split='train',\n",
    "                 data_dir='gs://tfds-data/datasets',\n",
    "                 sample_rate=16000,\n",
    "                 frame_rate=250,\n",
    "                 include_note_labels=True):\n",
    "        \"\"\"TfdsProvider constructor.\n",
    "        Args:\n",
    "          name: TFDS dataset name (with optional config and version).\n",
    "          split: Dataset split to use of the TFDS dataset.\n",
    "          data_dir: The directory to read the prepared NSynth dataset from. Defaults\n",
    "            to the public TFDS GCS bucket.\n",
    "          sample_rate: Sample rate of audio in the dataset.\n",
    "          frame_rate: Frame rate of features in the dataset.\n",
    "          include_note_labels: Return dataset without note-level labels\n",
    "            (pitch, instrument).\n",
    "        \"\"\"\n",
    "        self._include_note_labels = include_note_labels\n",
    "        if data_dir == 'gs://tfds-data/datasets':\n",
    "            logging.warning(\n",
    "                'Using public TFDS GCS bucket to load NSynth. If not running on '\n",
    "                'GCP, this will be very slow, and it is recommended you prepare '\n",
    "                'the dataset locally with TFDS and set the data_dir appropriately.')\n",
    "        super().__init__(name, split, data_dir, sample_rate, frame_rate)\n",
    "\n",
    "    def get_dataset(self, shuffle=True):\n",
    "        \"\"\"Returns dataset with slight restructuring of feature dictionary.\"\"\"\n",
    "        def preprocess_ex(ex):\n",
    "            ex_out = {\n",
    "                'audio':\n",
    "                    ex['audio'],\n",
    "                'f0_hz':\n",
    "                    ex['f0']['hz'],\n",
    "                'f0_confidence':\n",
    "                    ex['f0']['confidence'],\n",
    "                'loudness_db':\n",
    "                    ex['loudness']['db'],\n",
    "            }\n",
    "            if self._include_note_labels:\n",
    "                ex_out.update({\n",
    "                    'pitch':\n",
    "                        ex['pitch'],\n",
    "                    'instrument_source':\n",
    "                        ex['instrument']['source'],\n",
    "                    'instrument_family':\n",
    "                        ex['instrument']['family'],\n",
    "                    'instrument':\n",
    "                        ex['instrument']['label'],\n",
    "                })\n",
    "            return ex_out\n",
    "\n",
    "        dataset = super().get_dataset(shuffle)\n",
    "        dataset = dataset.map(preprocess_ex, num_parallel_calls=_AUTOTUNE)\n",
    "        return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "    def preprocess_ex(ex):\n",
    "        ex_out = {\n",
    "            'audio':\n",
    "                ex['audio'],\n",
    "            'f0_hz':\n",
    "                ex['f0']['hz'],\n",
    "            'f0_confidence':\n",
    "                ex['f0']['confidence'],\n",
    "            'loudness_db':\n",
    "                ex['loudness']['db'],\n",
    "        }\n",
    "        return ex_out\n",
    "\n",
    "\n",
    "processed = json.loads(MessageToJson(parsed_record))\n",
    "processed= processed['context']['feature']\n",
    "#instrument_label = processed['context']['feature']['instrument/label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [55]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# convert raw uninformative string to JSON and extract relevant features\u001B[39;00m\n\u001B[0;32m     23\u001B[0m processed \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(MessageToJson(parsed_record))\n\u001B[1;32m---> 24\u001B[0m processed\u001B[38;5;241m=\u001B[39m \u001B[43mprocessed\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     25\u001B[0m preprocess_ex(processed)\n",
      "Input \u001B[1;32mIn [55]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# convert raw uninformative string to JSON and extract relevant features\u001B[39;00m\n\u001B[0;32m     23\u001B[0m processed \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(MessageToJson(parsed_record))\n\u001B[1;32m---> 24\u001B[0m processed\u001B[38;5;241m=\u001B[39m \u001B[43mprocessed\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     25\u001B[0m preprocess_ex(processed)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import ddsp\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "ds = tf.data.TFRecordDataset(\n",
    "    filenames_full\n",
    ").batch(\n",
    "    my_batch_size\n",
    ")\n",
    "#data_provider.download_and_prepare()\n",
    "dataset_gan = tf.data.Dataset.from_tensor_slices(filenames_full)\n",
    "def read(inp):\n",
    "    return tf.data.TFRecordDataset(inp)\n",
    "\n",
    "file_content = dataset_gan.map(read)\n",
    "\n",
    "\n",
    "data_group_000 = tf.data.TFRecordDataset(r'E:\\\\nsynthgan_generation_sorted\\\\mallet\\\\train\\\\3-train-mallet-nsynth-subset-train.tfrecord-3-label-mallet')\n",
    "for raw_record in data_group_000.take(2):\n",
    "    parsed_record = tf.train.SequenceExample()\n",
    "    parsed_record.ParseFromString(raw_record.numpy())\n",
    "    # convert raw uninformative string to JSON and extract relevant features\n",
    "    processed = json.loads(MessageToJson(parsed_record))\n",
    "    processed= processed['context']['feature']\n",
    "    preprocess_ex(processed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parsing builder name string E:/nsynthgan_generation_sorted/mallet/train/3-train-mallet-nsynth-subset-train.tfrecord-3-label-mallet failed.\nThe builder name string must be of the following format:\n  dataset_name[/config_name][:version][/kwargs]\n\n  Where:\n\n    * dataset_name and config_name are string following python variable naming.\n    * version is of the form x.y.z where {x,y,z} can be any digit or *.\n    * kwargs is a comma list separated of arguments and values to pass to\n      builder.\n\n  Examples:\n    my_dataset\n    my_dataset:1.2.*\n    my_dataset/config1\n    my_dataset/config1:1.*.*\n    my_dataset/config1/arg1=val1,arg2=val2\n    my_dataset/config1:1.2.3/right=True,foo=bar,rate=1.2\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mE:/nsynthgan_generation_sorted/mallet/train/3-train-mallet-nsynth-subset-train.tfrecord-3-label-mallet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilenames_full\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:250\u001B[0m, in \u001B[0;36mload.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    248\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    252\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:572\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m builder_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    570\u001B[0m   builder_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 572\u001B[0m dbuilder \u001B[38;5;241m=\u001B[39m builder(name, data_dir\u001B[38;5;241m=\u001B[39mdata_dir, try_gcs\u001B[38;5;241m=\u001B[39mtry_gcs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m    574\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[1;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:283\u001B[0m, in \u001B[0;36mbuilder.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    281\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 283\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    285\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:154\u001B[0m, in \u001B[0;36mbuilder\u001B[1;34m(name, try_gcs, **builder_kwargs)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m\"\"\"Fetches a `tfds.core.DatasetBuilder` by string name.\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;124;03m  DatasetNotFoundError: if `name` is unrecognized.\u001B[39;00m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# 'kaggle:my_ds/config:1.0.0' -> (\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m#     DatasetName('kaggle:my_ds'), {'version': '1.0.0', 'config': 'conf0'}\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m name, builder_kwargs \u001B[38;5;241m=\u001B[39m naming\u001B[38;5;241m.\u001B[39mparse_builder_name_kwargs(\n\u001B[0;32m    155\u001B[0m     name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;66;03m# `try_gcs` currently only supports non-community datasets\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (try_gcs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m name\u001B[38;5;241m.\u001B[39mnamespace \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    159\u001B[0m     gcs_utils\u001B[38;5;241m.\u001B[39mis_dataset_on_gcs(\u001B[38;5;28mstr\u001B[39m(name))):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\naming.py:137\u001B[0m, in \u001B[0;36mparse_builder_name_kwargs\u001B[1;34m(name, **builder_kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_builder_name_kwargs\u001B[39m(\n\u001B[0;32m    113\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs: Any,\n\u001B[0;32m    115\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[DatasetName, Dict[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    116\u001B[0m   \u001B[38;5;124;03m\"\"\"Normalize builder kwargs.\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \n\u001B[0;32m    118\u001B[0m \u001B[38;5;124;03m  Example:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m    builder_kwargs: Builder kwargs (version, config, data_dir,...)\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m   name, parsed_builder_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43m_dataset_name_and_kwargs_from_name_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m   builder_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparsed_builder_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)\n\u001B[0;32m    139\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m DatasetName(name), builder_kwargs\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\naming.py:168\u001B[0m, in \u001B[0;36m_dataset_name_and_kwargs_from_name_str\u001B[1;34m(name_str)\u001B[0m\n\u001B[0;32m    166\u001B[0m res \u001B[38;5;241m=\u001B[39m _NAME_REG\u001B[38;5;241m.\u001B[39mmatch(name_str)\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m res:\n\u001B[1;32m--> 168\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err_msg)\n\u001B[0;32m    169\u001B[0m name \u001B[38;5;241m=\u001B[39m res\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset_name\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# Normalize the name to accept CamelCase\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Parsing builder name string E:/nsynthgan_generation_sorted/mallet/train/3-train-mallet-nsynth-subset-train.tfrecord-3-label-mallet failed.\nThe builder name string must be of the following format:\n  dataset_name[/config_name][:version][/kwargs]\n\n  Where:\n\n    * dataset_name and config_name are string following python variable naming.\n    * version is of the form x.y.z where {x,y,z} can be any digit or *.\n    * kwargs is a comma list separated of arguments and values to pass to\n      builder.\n\n  Examples:\n    my_dataset\n    my_dataset:1.2.*\n    my_dataset/config1\n    my_dataset/config1:1.*.*\n    my_dataset/config1/arg1=val1,arg2=val2\n    my_dataset/config1:1.2.3/right=True,foo=bar,rate=1.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfds.load(\n",
    "    name = r'E:/nsynthgan_generation_sorted/mallet/train/3-train-mallet-nsynth-subset-train.tfrecord-3-label-mallet',\n",
    "    data_dir=filenames_full,\n",
    "    split=\"train\",\n",
    "    shuffle_files=True,\n",
    "    download=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def parse_tfexample(record):\n",
    "    return tf.io.parse_single_example(record,features_dict)\n",
    "def features_dict():\n",
    "    base_features = {\n",
    "        'audio':\n",
    "            tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'f0_hz':\n",
    "            tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'f0_confidence':\n",
    "            tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'loudness_db':\n",
    "            tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input serialized must be a scalar",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m raw_record2 \u001B[38;5;129;01min\u001B[39;00m ds\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 3\u001B[0m     feature_dict_ex\u001B[38;5;241m=\u001B[39m\u001B[43mparse_tfexample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_record2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     parsed_record2 \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mSequenceExample()\n\u001B[0;32m      5\u001B[0m     feature_dict_ex\u001B[38;5;241m=\u001B[39mparse_tfexample(parsed_record2)\n",
      "Input \u001B[1;32mIn [44]\u001B[0m, in \u001B[0;36mparse_tfexample\u001B[1;34m(record)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_tfexample\u001B[39m(record):\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_single_example\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m,\u001B[49m\u001B[43mfeatures_dict\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py:1148\u001B[0m, in \u001B[0;36m_assert_scalar\u001B[1;34m(value, name)\u001B[0m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1148\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m must be a scalar\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m name)\n",
      "\u001B[1;31mValueError\u001B[0m: Input serialized must be a scalar"
     ]
    }
   ],
   "source": [
    "for raw_record2 in ds.take(1):\n",
    "\n",
    "    feature_dict_ex=parse_tfexample(raw_record2)\n",
    "    parsed_record2 = tf.train.SequenceExample()\n",
    "    feature_dict_ex=parse_tfexample(parsed_record2)\n",
    "    parsed_record2.ParseFromString(raw_record2.numpy())\n",
    "    # convert raw uninformative string to JSON and extract relevant features\n",
    "    processed2 = json.loads(MessageToJson(parsed_record))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
