{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beam\n",
      "  Downloading apache_beam-2.43.0-cp310-cp310-win_amd64.whl (4.5 MB)\n",
      "     ---------------------------------------- 4.5/4.5 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2018.3 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (2022.2.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (2.2.0)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.13.0-cp310-cp310-win_amd64.whl (394 kB)\n",
      "     -------------------------------------- 394.5/394.5 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (4.2.0)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.8.2-cp310-none-win_amd64.whl (200 kB)\n",
      "     -------------------------------------- 200.1/200.1 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (1.22.4)\n",
      "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (9.0.0)\n",
      "Collecting zstandard<1,>=0.18.0\n",
      "  Downloading zstandard-0.19.0-cp310-cp310-win_amd64.whl (473 kB)\n",
      "     -------------------------------------- 473.8/473.8 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (2.8.2)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "     -------------------------------------- 152.0/152.0 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fasteners<1.0,>=0.3\n",
      "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "     ---------------------------------------- 89.7/89.7 kB 2.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (2.27.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (1.4.2)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (1.44.0)\n",
      "Requirement already satisfied: httplib2<0.21.0,>=0.8 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (0.19.1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (2022.3.2)\n",
      "Requirement already satisfied: protobuf<4,>3.12.2 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\site-packages (from apache_beam) (3.19.6)\n",
      "Collecting objsize<0.6.0,>=0.5.2\n",
      "  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.7.0-cp310-cp310-win_amd64.whl (440 kB)\n",
      "     -------------------------------------- 440.8/440.8 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from apache_beam) (1.22.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from grpcio!=1.48.0,<2,>=1.33.1->apache_beam) (1.16.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from httplib2<0.21.0,>=0.8->apache_beam) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nico\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.12)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py): started\n",
      "  Building wheel for crcmod (setup.py): finished with status 'done'\n",
      "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-win_amd64.whl size=25076 sha256=d17ab1878a9215a22920e220459a37777436bd6e7086376a34bc033682892291\n",
      "  Stored in directory: c:\\users\\nico\\appdata\\local\\pip\\cache\\wheels\\04\\f7\\53\\82c3366abddfdacf8e5440e3ffdbd756a8dfeff7f6331e0667\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78628 sha256=e41a089e8eb79a12597143a8133a08a377b488e3257ae93571bac1381223068a\n",
      "  Stored in directory: c:\\users\\nico\\appdata\\local\\pip\\cache\\wheels\\31\\c9\\a2\\5388dc1514263c95ea7f89f48725b043ce2a5eb9c807681b0d\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=71bbb0edbfbf75b1e79e5b599591f3750592c46d04b8897bb7437fbb1bb892b9\n",
      "  Stored in directory: c:\\users\\nico\\appdata\\local\\pip\\cache\\wheels\\7c\\d7\\8d\\2156234738063e3d4a39ba77dc677046100e62766b53807189\n",
      "Successfully built crcmod dill docopt\n",
      "Installing collected packages: docopt, crcmod, zstandard, pymongo, orjson, objsize, fasteners, fastavro, dill, hdfs, apache_beam\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed apache_beam-2.43.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 hdfs-2.7.0 objsize-0.5.2 orjson-3.8.2 pymongo-3.13.0 zstandard-0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.0 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\n",
      "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install apache_beam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_datasets.audio import nsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'The NSynth Dataset is an audio dataset containing ~300k musical notes, each\\nwith a unique pitch, timbre, and envelope. Each note is annotated with three\\nadditional pieces of information based on a combination of human evaluation\\nand heuristic algorithms: Source, Family, and Qualities.\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsynth._AUDIO_RATE\n",
    "\n",
    "nsynth._DESCRIPTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['train', 'valid', 'test']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsynth._SPLITS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensorflow_datasets.audio.nsynth.NsynthConfig"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsynth.NsynthConfig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['abstract_reasoning',\n 'accentdb',\n 'aeslc',\n 'aflw2k3d',\n 'ag_news_subset',\n 'ai2_arc',\n 'ai2_arc_with_ir',\n 'amazon_us_reviews',\n 'anli',\n 'answer_equivalence',\n 'arc',\n 'asqa',\n 'asset',\n 'assin2',\n 'bair_robot_pushing_small',\n 'bccd',\n 'beans',\n 'bee_dataset',\n 'beir',\n 'big_patent',\n 'bigearthnet',\n 'billsum',\n 'binarized_mnist',\n 'binary_alpha_digits',\n 'ble_wind_field',\n 'blimp',\n 'booksum',\n 'bool_q',\n 'bucc',\n 'c4',\n 'caltech101',\n 'caltech_birds2010',\n 'caltech_birds2011',\n 'cardiotox',\n 'cars196',\n 'cassava',\n 'cats_vs_dogs',\n 'celeb_a',\n 'celeb_a_hq',\n 'cfq',\n 'cherry_blossoms',\n 'chexpert',\n 'cifar10',\n 'cifar100',\n 'cifar100_n',\n 'cifar10_1',\n 'cifar10_corrupted',\n 'cifar10_n',\n 'citrus_leaves',\n 'cityscapes',\n 'civil_comments',\n 'clevr',\n 'clic',\n 'clinc_oos',\n 'cmaterdb',\n 'cnn_dailymail',\n 'coco',\n 'coco_captions',\n 'coil100',\n 'colorectal_histology',\n 'colorectal_histology_large',\n 'common_voice',\n 'conll2003',\n 'controlled_noisy_web_labels',\n 'coqa',\n 'cos_e',\n 'cosmos_qa',\n 'covid19',\n 'covid19sum',\n 'crema_d',\n 'criteo',\n 'cs_restaurants',\n 'curated_breast_imaging_ddsm',\n 'cycle_gan',\n 'd4rl_adroit_door',\n 'd4rl_adroit_hammer',\n 'd4rl_adroit_pen',\n 'd4rl_adroit_relocate',\n 'd4rl_antmaze',\n 'd4rl_mujoco_ant',\n 'd4rl_mujoco_halfcheetah',\n 'd4rl_mujoco_hopper',\n 'd4rl_mujoco_walker2d',\n 'dart',\n 'davis',\n 'deep1b',\n 'deep_weeds',\n 'definite_pronoun_resolution',\n 'dementiabank',\n 'diabetic_retinopathy_detection',\n 'diamonds',\n 'div2k',\n 'dmlab',\n 'doc_nli',\n 'dolphin_number_word',\n 'domainnet',\n 'downsampled_imagenet',\n 'drop',\n 'dsprites',\n 'dtd',\n 'duke_ultrasound',\n 'e2e_cleaned',\n 'efron_morris75',\n 'emnist',\n 'eraser_multi_rc',\n 'esnli',\n 'eurosat',\n 'fashion_mnist',\n 'flic',\n 'flores',\n 'food101',\n 'forest_fires',\n 'fuss',\n 'gap',\n 'geirhos_conflict_stimuli',\n 'gem',\n 'genomics_ood',\n 'german_credit_numeric',\n 'gigaword',\n 'glove100_angular',\n 'glue',\n 'goemotions',\n 'gov_report',\n 'gpt3',\n 'gref',\n 'groove',\n 'grounded_scan',\n 'gsm8k',\n 'gtzan',\n 'gtzan_music_speech',\n 'hellaswag',\n 'higgs',\n 'hillstrom',\n 'horses_or_humans',\n 'howell',\n 'i_naturalist2017',\n 'i_naturalist2018',\n 'i_naturalist2021',\n 'imagenet2012',\n 'imagenet2012_corrupted',\n 'imagenet2012_fewshot',\n 'imagenet2012_multilabel',\n 'imagenet2012_real',\n 'imagenet2012_subset',\n 'imagenet_a',\n 'imagenet_lt',\n 'imagenet_r',\n 'imagenet_resized',\n 'imagenet_sketch',\n 'imagenet_v2',\n 'imagenette',\n 'imagewang',\n 'imdb_reviews',\n 'irc_disentanglement',\n 'iris',\n 'istella',\n 'kddcup99',\n 'kitti',\n 'kmnist',\n 'lambada',\n 'lfw',\n 'librispeech',\n 'librispeech_lm',\n 'libritts',\n 'ljspeech',\n 'lm1b',\n 'locomotion',\n 'lost_and_found',\n 'lsun',\n 'lvis',\n 'malaria',\n 'math_dataset',\n 'math_qa',\n 'mctaco',\n 'media_sum',\n 'mlqa',\n 'mnist',\n 'mnist_corrupted',\n 'movie_lens',\n 'movie_rationales',\n 'movielens',\n 'moving_mnist',\n 'mrqa',\n 'mslr_web',\n 'mt_opt',\n 'mtnt',\n 'multi_news',\n 'multi_nli',\n 'multi_nli_mismatch',\n 'natural_questions',\n 'natural_questions_open',\n 'newsroom',\n 'nsynth',\n 'nyu_depth_v2',\n 'ogbg_molpcba',\n 'omniglot',\n 'open_images_challenge2019_detection',\n 'open_images_v4',\n 'openbookqa',\n 'opinion_abstracts',\n 'opinosis',\n 'opus',\n 'oxford_flowers102',\n 'oxford_iiit_pet',\n 'para_crawl',\n 'pass',\n 'patch_camelyon',\n 'paws_wiki',\n 'paws_x_wiki',\n 'penguins',\n 'pet_finder',\n 'pg19',\n 'piqa',\n 'places365_small',\n 'placesfull',\n 'plant_leaves',\n 'plant_village',\n 'plantae_k',\n 'protein_net',\n 'qa4mre',\n 'qasc',\n 'quac',\n 'quality',\n 'quickdraw_bitmap',\n 'race',\n 'radon',\n 'reddit',\n 'reddit_disentanglement',\n 'reddit_tifu',\n 'ref_coco',\n 'resisc45',\n 'rlu_atari',\n 'rlu_atari_checkpoints',\n 'rlu_atari_checkpoints_ordered',\n 'rlu_control_suite',\n 'rlu_dmlab_explore_object_rewards_few',\n 'rlu_dmlab_explore_object_rewards_many',\n 'rlu_dmlab_rooms_select_nonmatching_object',\n 'rlu_dmlab_rooms_watermaze',\n 'rlu_dmlab_seekavoid_arena01',\n 'rlu_locomotion',\n 'rlu_rwrl',\n 'robomimic_ph',\n 'robonet',\n 'robosuite_panda_pick_place_can',\n 'rock_paper_scissors',\n 'rock_you',\n 's3o4d',\n 'salient_span_wikipedia',\n 'samsum',\n 'savee',\n 'scan',\n 'scene_parse150',\n 'schema_guided_dialogue',\n 'sci_tail',\n 'scicite',\n 'scientific_papers',\n 'scrolls',\n 'sentiment140',\n 'shapes3d',\n 'sift1m',\n 'simpte',\n 'siscore',\n 'smallnorb',\n 'smartwatch_gestures',\n 'snli',\n 'so2sat',\n 'speech_commands',\n 'spoken_digit',\n 'squad',\n 'squad_question_generation',\n 'stanford_dogs',\n 'stanford_online_products',\n 'star_cfq',\n 'starcraft_video',\n 'stl10',\n 'story_cloze',\n 'summscreen',\n 'sun397',\n 'super_glue',\n 'svhn_cropped',\n 'symmetric_solids',\n 'tao',\n 'tatoeba',\n 'ted_hrlr_translate',\n 'ted_multi_translate',\n 'tedlium',\n 'tf_flowers',\n 'the300w_lp',\n 'tiny_shakespeare',\n 'titanic',\n 'trec',\n 'trivia_qa',\n 'tydi_qa',\n 'uc_merced',\n 'ucf101',\n 'unified_qa',\n 'universal_dependencies',\n 'user_libri_audio',\n 'user_libri_text',\n 'vctk',\n 'visual_domain_decathlon',\n 'voc',\n 'voxceleb',\n 'voxforge',\n 'waymo_open_dataset',\n 'web_graph',\n 'web_nlg',\n 'web_questions',\n 'wider_face',\n 'wiki40b',\n 'wiki_auto',\n 'wiki_bio',\n 'wiki_dialog',\n 'wiki_table_questions',\n 'wiki_table_text',\n 'wikiann',\n 'wikihow',\n 'wikipedia',\n 'wikipedia_toxicity_subtypes',\n 'wine_quality',\n 'winogrande',\n 'wit',\n 'wit_kaggle',\n 'wmt13_translate',\n 'wmt14_translate',\n 'wmt15_translate',\n 'wmt16_translate',\n 'wmt17_translate',\n 'wmt18_translate',\n 'wmt19_translate',\n 'wmt_t2t_translate',\n 'wmt_translate',\n 'wordnet',\n 'wsc273',\n 'xnli',\n 'xquad',\n 'xsum',\n 'xtreme_pawsx',\n 'xtreme_pos',\n 'xtreme_s',\n 'xtreme_xnli',\n 'yahoo_ltrc',\n 'yelp_polarity_reviews',\n 'yes_no',\n 'youtube_vis']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to E:\\NSYNTH_DATASET\\nsynth\\full\\2.3.3...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d11c57a0a584555858624f609668cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db4efac7260b43b0931948be9ab3f298"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bf3eb39531d45ce867e93c2b8d79348"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apache_beam'\nFailed importing apache_beam. This likely means that the dataset requires additional dependencies that have to be manually installed (usually with `pip install apache_beam`). See setup.py extras_require.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\lazy_imports_lib.py:30\u001B[0m, in \u001B[0;36m_try_import\u001B[1;34m(module_name)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 30\u001B[0m   mod \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m mod\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1004\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'apache_beam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnsynth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mE:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mNSYNTH_DATASET\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(ds)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:250\u001B[0m, in \u001B[0;36mload.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    248\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    252\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:575\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m    574\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m--> 575\u001B[0m   dbuilder\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_dataset_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    578\u001B[0m   as_dataset_kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:523\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, download_dir, download_config, file_format)\u001B[0m\n\u001B[0;32m    521\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mread_from_directory(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir)\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 523\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    528\u001B[0m   \u001B[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001B[39;00m\n\u001B[0;32m    529\u001B[0m   \u001B[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001B[39;00m\n\u001B[0;32m    530\u001B[0m   \u001B[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001B[39;00m\n\u001B[0;32m    531\u001B[0m   \u001B[38;5;66;03m# when reading from package data.\u001B[39;00m\n\u001B[0;32m    532\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdownload_size \u001B[38;5;241m=\u001B[39m dl_manager\u001B[38;5;241m.\u001B[39mdownloaded_size\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1249\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, download_config)\u001B[0m\n\u001B[0;32m   1244\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_generators(  \u001B[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001B[39;00m\n\u001B[0;32m   1245\u001B[0m     dl_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptional_pipeline_kwargs)\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001B[39;00m\n\u001B[0;32m   1247\u001B[0m \u001B[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001B[39;00m\n\u001B[0;32m   1248\u001B[0m \u001B[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001B[39;00m\n\u001B[1;32m-> 1249\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[43msplit_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize_legacy_split_generators\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1250\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit_generators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_generators\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_examples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_beam\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBeamBasedBuilder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1253\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1255\u001B[0m \u001B[38;5;66;03m# Ensure `all` isn't used as key.\u001B[39;00m\n\u001B[0;32m   1256\u001B[0m _check_split_names(split_generators\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:267\u001B[0m, in \u001B[0;36mSplitBuilder.normalize_legacy_split_generators\u001B[1;34m(self, split_generators, generator_fn, is_beam)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(split_generators, \u001B[38;5;28mlist\u001B[39m):  \u001B[38;5;66;03m# Legacy structure\u001B[39;00m\n\u001B[0;32m    266\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m is_beam:  \u001B[38;5;66;03m# Legacy `tfds.core.BeamBasedBuilder`\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m     beam \u001B[38;5;241m=\u001B[39m \u001B[43mlazy_imports_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlazy_imports\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapache_beam\u001B[49m\n\u001B[0;32m    268\u001B[0m     generator_fn \u001B[38;5;241m=\u001B[39m beam\u001B[38;5;241m.\u001B[39mptransform_fn(generator_fn)\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    270\u001B[0m         s\u001B[38;5;241m.\u001B[39mname: generator_fn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39ms\u001B[38;5;241m.\u001B[39mgen_kwargs)  \u001B[38;5;66;03m# Create the `beam.PTransform`\u001B[39;00m\n\u001B[0;32m    271\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m split_generators\n\u001B[0;32m    272\u001B[0m     }\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:153\u001B[0m, in \u001B[0;36mclassproperty.__get__\u001B[1;34m(self, obj, objtype)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__get__\u001B[39m(\u001B[38;5;28mself\u001B[39m, obj, objtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__get__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobjtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\lazy_imports_lib.py:51\u001B[0m, in \u001B[0;36mLazyImporter.apache_beam\u001B[1;34m(cls)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;129m@utils\u001B[39m\u001B[38;5;241m.\u001B[39mclassproperty\n\u001B[0;32m     49\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapache_beam\u001B[39m(\u001B[38;5;28mcls\u001B[39m):\n\u001B[1;32m---> 51\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_try_import\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mapache_beam\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\lazy_imports_lib.py:37\u001B[0m, in \u001B[0;36m_try_import\u001B[1;34m(module_name)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     33\u001B[0m   err_msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed importing \u001B[39m\u001B[38;5;132;01m{name}\u001B[39;00m\u001B[38;5;124m. This likely means that the dataset \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     34\u001B[0m              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires additional dependencies that have to be \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmanually installed (usually with `pip install \u001B[39m\u001B[38;5;132;01m{name}\u001B[39;00m\u001B[38;5;124m`). See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     36\u001B[0m              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup.py extras_require.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mmodule_name)\n\u001B[1;32m---> 37\u001B[0m   \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merr_msg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:381\u001B[0m, in \u001B[0;36mreraise\u001B[1;34m(e, prefix, suffix)\u001B[0m\n\u001B[0;32m    379\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    380\u001B[0m     exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(e)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 381\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# Otherwise, modify the exception in-place\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(e\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'apache_beam'\nFailed importing apache_beam. This likely means that the dataset requires additional dependencies that have to be manually installed (usually with `pip install apache_beam`). See setup.py extras_require."
     ]
    }
   ],
   "source": [
    "\n",
    "ds = tfds.load('nsynth', data_dir=r'E:\\NSYNTH_DATASET', shuffle_files=True)\n",
    "print(ds)\n",
    "\n",
    "#ds.save(\"nsynth_dataset\"\n",
    "    #, compression=None, shard_func=None, checkpoint_args=None\n",
    "#)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
